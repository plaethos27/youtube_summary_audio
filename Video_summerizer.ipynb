{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlfoduKR8NqD9ArLR8g8kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaethos27/youtube_summary_audio/blob/main/Video_summerizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "\n",
        "from pytube import YouTube\n",
        "import whisper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "188GMHQaRUMi",
        "outputId": "18b5ea1c-6ac7-449a-d40c-3d7334fd10f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-vn0in1s8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-vn0in1s8\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=0ed31f80c0fa55a94d95c29758f1253e3cf4e5397a7809f40067059b98f63e5e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-heos5ket/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the video\n",
        "yt = YouTube('https://www.youtube.com/watch?v=mSsDNGWZ7Ao')  # replace with your video URL\n",
        "stream = yt.streams.filter(only_audio=True).first()\n",
        "stream.download(filename='FILE_NAME.mp3')  # replace with the name you want to save the file as\n",
        "\n",
        "# Load the model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe the audio\n",
        "result = model.transcribe('FILE_NAME.mp3')  # replace with the name of the downloaded file\n",
        "\n",
        "# Print the transcription\n",
        "transcript = result[\"text\"]\n",
        "print(transcript)\n",
        "\n",
        "# Write the transcription to a text file\n",
        "with open('transcript.txt', 'w') as f:\n",
        "    f.write(transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l9BbePtTXVW",
        "outputId": "3a743e10-75f8-4a7e-e4fd-ae886190b1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 91.4MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello and welcome to this tutorial on TensorFlow in 10 minutes. So what's in it for you? First, we'll be looking at what exactly TensorFlow is and by whom it was developed. Then we'll take a look at tensors, the basic foundational building blocks of TensorFlow. Then we'll look at some features of TensorFlow which make it so desirable. Finally, we'll be looking at some companies which use TensorFlow in their day-to-day working and we look at the different applications in which they use TensorFlow. Then we'll be taking a look at neural networks with TensorFlow. So let's get started. First, what exactly is TensorFlow? TensorFlow is nothing but a Python software library which was created by Google to help implement large-scale machine learning models and to help solve complex numerical problems. It was developed by the Google AI team and TensorFlow helps us implement machine learning using Python while keeping the mathematical computation of the entire process in C++, hence helping us calculate complex numerical problems faster. Machine learning is a very daunting task and using TensorFlow, we can streamline the process of acquiring data, changing our models, serving predictions and refining future results in a very easy and fast way. TensorFlow is used by Google itself in all Google services which use AI and to help optimize the Google search bar. TensorFlow helps in sorting through huge amounts of data to help find the relevant search results. Now what exactly are tensors? Tensors are nothing but simply containers which are used to hold data in the form of matrices. The tensors can be of any dimension and using tensors we can perform linear operations on vast quantities of data. Now tensors can hold data in any dimension as I said, they can hold it in 3D space too. Tensors are nothing but matrices and matrices as we know can be of one dimension which means that it has a single column and multiple rows, two dimension which means it has multiple columns and multiple rows or three dimension which means along with having rows and columns, it has multiple rows and columns start behind one another. This makes holding vast quantities of data very simple intensive flow and performing matrix calculations on these huge quantities of data very simple. Using tensors we can perform dot product as well as cross product easily on three dimension tensors. The next topic that we'll be dealing with is features of TensorFlow. One of the main features of TensorFlow is that it's an open source library which means that anyone can use TensorFlow as long as they have an internet connection. TensorFlow can manipulate TensorFlow in ways which you can't even think of and come up with amazing and new products. It has become a DIY community and it has a huge forum for people who are getting started with it and for those who find it hard to use it or for those who need help with their work. It also has a really good documentation, a lot of people who are already working on TensorFlow which means that you have a lot of support available from around the world. Another feature of TensorFlow is that it has a large community of users. It's been developed by Google so it already has an amazing software engineering team who works on stability improvements continuously. Using TensorFlow you can train multiple neural networks parallelly. TensorFlow offers pipe lining which means that you can train multiple neural networks and multiple GPUs at the same time. TensorFlow also offers in depth graph visualization. In TensorFlow you don't really have a separate graph method to create graphs. Instead you have computational graphs which are built in processes which use the library without needing to call a graph object directly. A graph object in TensorFlow can be created as a result of a simple line of code. Like let's say we take two tensors and we add them up. This will create an operation node that will take two tensors and produce the sum and create a graph automatically of that. Another feature is that TensorFlow has adopted Keras for high level APIs. Keras is only an extension for making it easier to read and write machine learning programs. While TensorFlow is an open source library, Keras is simply a neural network library. TensorFlow will provide both high level and low level APIs while Keras provides only high level APIs which work with neural networks. It's also easy to train a machine learning model on both CPUs and GPUs using TensorFlow. Now let's take a look at some of the companies which use TensorFlow and the various operations in which they've implemented TensorFlow. First company that we'll be looking at is Airbnb. The Airbnb Engineering and Data Science team has applied machine learning using TensorFlow to classify images and detect objects at scale and helping in improving the guest experience. Machine learning with TensorFlow enabled mobile proof of purchase at Coca-Cola. Advances in artificial intelligence and the maturity of TensorFlow enabled the Coca-Cola company to finally achieve a long-sought after frictionless proof of purchase capability on their mobile app by using TensorFlow. Airbus also uses TensorFlow and they use it to extract information from satellite images and deliver valuable insights to their clients. Machine learning helps with monitoring changes with the Earth's surface for urban planning, providing illegal construction and mapping damage and landscape changes which are caused by natural catastrophes. All of this is used by Airbus to gather statistics and to deliver valuable insights. Internet has partnered with Google to optimize TensorFlow inference performance across different models. This work has resulted in up to 2.8 times performance improvement which benefits the TensorFlow community and a wide range of customers using TensorFlow on Intel platforms. Nepal is also using TensorFlow and it's using it to stay at cutting edge of fraud detection. Using TensorFlow deep transfer learning and generative modeling, PayPal has been able to recognize complex, temporarily varying fraud patterns to help increase their fraud decline accuracy while improving experience of legitimate users through increased precision and identification. And finally Lenovo. Lenovo's intelligent computing orchestration is using TensorFlow to help accelerate the intelligent revolution. The Lenovo, leco platform which is nothing but the Lenovo intelligent computing orchestration platform accelerates AI training and traditional high performance computing and optimizes deep learning training with TensorFlow integration and optimization. Leco provides various built-in TensorFlow models and supports optimized distributed training of these models. Now let's dive into neural networks with TensorFlow. Before we really start neural networks, let's see how computation is done in TensorFlow. In TensorFlow, all numerical calculations are done using computational graphs. Computational graphs are nothing more than graphs where each node will correspond to a new mathematical operation. These mathematical operations will be performed on the input tensors. Computational graphs are a way of expressing and evaluating a mathematical operation. They further allow TensorFlow to perform lazy computing. What is lazy computing exactly? lazy computing is nothing more than representing computation without actually performing it until it is asked. To create computational graphs, the first thing that we're going to do is represent our data flow, which is nothing more than the way our data is flowing in our program in the form of a graph. On the screen here, we have a very simple graph. We have our data x, y and a constant two, which is being fed into our operators, which are the nodes of a graph. The operators here are multiplication and addition. The data that we're inputting to our graph is in the form of tensors. Even two, which is nothing more than a constant in this case, will have to be saved as a tensor before we can compute on it. The tensors are the input to our graphs, and the nodes will perform various mathematical operations on our tensors. Now one of the main advantages of having a graph is flexibility. When you create a graph, you're not bound to run the whole graph. In this case, I can run only a single node, let's say this multiplication between x and y without really having to perform addition between y and 2. This allows flexibility to a certain level. We can control parts of the graphs and we can execute them separately. And one of the biggest advantages of tens of flow further is the visualization of these computational graphs. To run a part of the graph is called running a session. This is the second part of a computational graph. After creating a graph, we want to run it. But we might not want to run the entire graph. We might only want to run a part of it. To run a part of the graph, we have to create a session. To compute anything, a graph must be launched in a session. Technically, sessions place the graph outputs on hardware such as CPUs or GPUs and provides method to execute them. Session objects are created and then using a run method in our code, we will run enough of the computational graph to get a desired output. That means that we only run which part of the graph is absolutely necessary to run. In the case of this example, we only want to get our x multiplied by y and our y plus 2. So we're only running the bottom two nodes of our graph without really running the top part. This will not only save computational part, it will also increase efficiency of our computation. Now let's look into neural networks. The first question that comes into mind is what exactly a neural networks? Neural networks are a set of artificial neurons with multiple layers that's designed to mimic how the human brain works. A neural network will contain layers of interconnected nodes. All the circles in our diagram represent the various nodes of our neural networks which are connected to one another. Neural networks are in the form of layers. The first layer is our input layer which contains nothing more than our input tensors. The input tensors will each have a weight associated with them depending on how important they are to our output layer. They did layer in the middle will help find you in the input weightings until the neural networks margin of error is minimal. Each node in our neural network can be taken as a perceptron and is similar to a multiple linear regression. The perceptron feeds the signal which is produced by multiple linear regressions into an activation function which may be nonlinear. The activation function is present within our nodes and depending on the activation function we decide the weightage of our node. Now in tens of flow we know that our data is saved in the form of tensors. These tensors are the input to all of our nodes and all of the tensors are implemented in the form of Python. The computation which is taking place within our nodes, the activation functions and the hidden layer functions are all implemented in C++. The reason we use C++ is because C++ is relatively faster than Python. This makes implementing our neural network using tensors flow a very fast process. Thank you for watching this video. For more videos like this and for more information please visit www.simpliland.com.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the transcripts dataset\n",
        "transcripts_df = pd.read_csv('transcripts.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0OetPF-jS1j",
        "outputId": "eca49870-f6e5-4c5f-c4d1-12e591b3170d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the transcripts to get a list of sentences\n",
        "def preprocess_transcripts(transcripts):\n",
        "    # Load stopwords set\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = []\n",
        "    for transcript in transcripts:\n",
        "        # Tokenize the transcript into sentences\n",
        "        transcript_sentences = sent_tokenize(transcript)\n",
        "        for sentence in transcript_sentences:\n",
        "            # Lowercase the sentence\n",
        "            sentence = sentence.lower()\n",
        "            # Remove punctuation\n",
        "            sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "            # Remove stopwords\n",
        "            sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "sentences = preprocess_transcripts(transcripts_df['transcript'])\n",
        "\n",
        "# Download the pre-trained Word2Vec model\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "vdTWelkTpEx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create sentence embeddings by averaging word embeddings\n",
        "def sentence_embedding(sentence, word_vectors):\n",
        "    words = word_tokenize(sentence)\n",
        "    embeddings = [word_vectors[word] for word in words if word in word_vectors]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word_vectors.vector_size)\n",
        "\n",
        "# Create sentence embeddings for each sentence\n",
        "sentence_embeddings = np.array([sentence_embedding(sentence, word_vectors) for sentence in sentences])\n",
        "\n",
        "# Define your labels here. This is a placeholder. You should replace this with your own labeling strategy.\n",
        "def define_labels(sentences):\n",
        "    # Example labeling strategy: label sentences as 'important' (1) or 'not important' (0) based on their length.\n",
        "    return [1 if len(sentence.split()) > 7 else 0 for sentence in sentences]\n",
        "\n",
        "labels = define_labels(sentences)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "D57dbnW9sWlF",
        "outputId": "e8268204-e962-433d-b077-e89723df8865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the new transcript from 'transcript.txt'\n",
        "with open('transcript.txt', 'r') as file:\n",
        "    new_transcript = file.read()\n",
        "\n",
        "# Tokenize the new transcript into sentences\n",
        "new_sentences = sent_tokenize(new_transcript)\n",
        "\n",
        "# Create sentence embeddings for the new sentences\n",
        "new_sentence_embeddings = np.array([sentence_embedding(sentence, word_vectors) for sentence in new_sentences])\n",
        "\n",
        "# Predict the 'importance' of each sentence\n",
        "predictions = model.predict(new_sentence_embeddings)\n",
        "\n",
        "# Create a summary by selecting the 'important' sentences\n",
        "summary_sentences = [sentence for sentence, prediction in zip(new_sentences, predictions) if prediction == 1]\n",
        "summary = ' '.join(summary_sentences)\n",
        "\n",
        "# Save the summary to a text file\n",
        "with open('summary.txt', 'w') as file:\n",
        "    file.write(summary)\n",
        "\n",
        "# Output the summary to the console as well (optional)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50-rJVuCs1Br",
        "outputId": "b547473f-e9a4-46bd-f7cc-87d9faeaa614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So what's in it for you? First, we'll be looking at what exactly TensorFlow is and by whom it was developed. Then we'll take a look at tensors, the basic foundational building blocks of TensorFlow. Then we'll look at some features of TensorFlow which make it so desirable. Finally, we'll be looking at some companies which use TensorFlow in their day-to-day working and we look at the different applications in which they use TensorFlow. Then we'll be taking a look at neural networks with TensorFlow. So let's get started. TensorFlow is nothing but a Python software library which was created by Google to help implement large-scale machine learning models and to help solve complex numerical problems. It was developed by the Google AI team and TensorFlow helps us implement machine learning using Python while keeping the mathematical computation of the entire process in C++, hence helping us calculate complex numerical problems faster. Machine learning is a very daunting task and using TensorFlow, we can streamline the process of acquiring data, changing our models, serving predictions and refining future results in a very easy and fast way. TensorFlow is used by Google itself in all Google services which use AI and to help optimize the Google search bar. TensorFlow helps in sorting through huge amounts of data to help find the relevant search results. Tensors are nothing but simply containers which are used to hold data in the form of matrices. The tensors can be of any dimension and using tensors we can perform linear operations on vast quantities of data. Now tensors can hold data in any dimension as I said, they can hold it in 3D space too. Tensors are nothing but matrices and matrices as we know can be of one dimension which means that it has a single column and multiple rows, two dimension which means it has multiple columns and multiple rows or three dimension which means along with having rows and columns, it has multiple rows and columns start behind one another. This makes holding vast quantities of data very simple intensive flow and performing matrix calculations on these huge quantities of data very simple. Using tensors we can perform dot product as well as cross product easily on three dimension tensors. The next topic that we'll be dealing with is features of TensorFlow. One of the main features of TensorFlow is that it's an open source library which means that anyone can use TensorFlow as long as they have an internet connection. TensorFlow can manipulate TensorFlow in ways which you can't even think of and come up with amazing and new products. It has become a DIY community and it has a huge forum for people who are getting started with it and for those who find it hard to use it or for those who need help with their work. It also has a really good documentation, a lot of people who are already working on TensorFlow which means that you have a lot of support available from around the world. Another feature of TensorFlow is that it has a large community of users. It's been developed by Google so it already has an amazing software engineering team who works on stability improvements continuously. Using TensorFlow you can train multiple neural networks parallelly. TensorFlow offers pipe lining which means that you can train multiple neural networks and multiple GPUs at the same time. TensorFlow also offers in depth graph visualization. In TensorFlow you don't really have a separate graph method to create graphs. Instead you have computational graphs which are built in processes which use the library without needing to call a graph object directly. A graph object in TensorFlow can be created as a result of a simple line of code. Like let's say we take two tensors and we add them up. This will create an operation node that will take two tensors and produce the sum and create a graph automatically of that. Another feature is that TensorFlow has adopted Keras for high level APIs. While TensorFlow is an open source library, Keras is simply a neural network library. TensorFlow will provide both high level and low level APIs while Keras provides only high level APIs which work with neural networks. It's also easy to train a machine learning model on both CPUs and GPUs using TensorFlow. Now let's take a look at some of the companies which use TensorFlow and the various operations in which they've implemented TensorFlow. First company that we'll be looking at is Airbnb. The Airbnb Engineering and Data Science team has applied machine learning using TensorFlow to classify images and detect objects at scale and helping in improving the guest experience. Machine learning with TensorFlow enabled mobile proof of purchase at Coca-Cola. Advances in artificial intelligence and the maturity of TensorFlow enabled the Coca-Cola company to finally achieve a long-sought after frictionless proof of purchase capability on their mobile app by using TensorFlow. Airbus also uses TensorFlow and they use it to extract information from satellite images and deliver valuable insights to their clients. Machine learning helps with monitoring changes with the Earth's surface for urban planning, providing illegal construction and mapping damage and landscape changes which are caused by natural catastrophes. All of this is used by Airbus to gather statistics and to deliver valuable insights. Internet has partnered with Google to optimize TensorFlow inference performance across different models. This work has resulted in up to 2.8 times performance improvement which benefits the TensorFlow community and a wide range of customers using TensorFlow on Intel platforms. Nepal is also using TensorFlow and it's using it to stay at cutting edge of fraud detection. Using TensorFlow deep transfer learning and generative modeling, PayPal has been able to recognize complex, temporarily varying fraud patterns to help increase their fraud decline accuracy while improving experience of legitimate users through increased precision and identification. Lenovo's intelligent computing orchestration is using TensorFlow to help accelerate the intelligent revolution. The Lenovo, leco platform which is nothing but the Lenovo intelligent computing orchestration platform accelerates AI training and traditional high performance computing and optimizes deep learning training with TensorFlow integration and optimization. Leco provides various built-in TensorFlow models and supports optimized distributed training of these models. Now let's dive into neural networks with TensorFlow. Before we really start neural networks, let's see how computation is done in TensorFlow. In TensorFlow, all numerical calculations are done using computational graphs. Computational graphs are nothing more than graphs where each node will correspond to a new mathematical operation. These mathematical operations will be performed on the input tensors. Computational graphs are a way of expressing and evaluating a mathematical operation. They further allow TensorFlow to perform lazy computing. lazy computing is nothing more than representing computation without actually performing it until it is asked. To create computational graphs, the first thing that we're going to do is represent our data flow, which is nothing more than the way our data is flowing in our program in the form of a graph. On the screen here, we have a very simple graph. We have our data x, y and a constant two, which is being fed into our operators, which are the nodes of a graph. The operators here are multiplication and addition. The data that we're inputting to our graph is in the form of tensors. Even two, which is nothing more than a constant in this case, will have to be saved as a tensor before we can compute on it. The tensors are the input to our graphs, and the nodes will perform various mathematical operations on our tensors. Now one of the main advantages of having a graph is flexibility. When you create a graph, you're not bound to run the whole graph. In this case, I can run only a single node, let's say this multiplication between x and y without really having to perform addition between y and 2. This allows flexibility to a certain level. We can control parts of the graphs and we can execute them separately. And one of the biggest advantages of tens of flow further is the visualization of these computational graphs. This is the second part of a computational graph. After creating a graph, we want to run it. But we might not want to run the entire graph. We might only want to run a part of it. To run a part of the graph, we have to create a session. To compute anything, a graph must be launched in a session. Technically, sessions place the graph outputs on hardware such as CPUs or GPUs and provides method to execute them. Session objects are created and then using a run method in our code, we will run enough of the computational graph to get a desired output. In the case of this example, we only want to get our x multiplied by y and our y plus 2. So we're only running the bottom two nodes of our graph without really running the top part. This will not only save computational part, it will also increase efficiency of our computation. Now let's look into neural networks. The first question that comes into mind is what exactly a neural networks? Neural networks are a set of artificial neurons with multiple layers that's designed to mimic how the human brain works. A neural network will contain layers of interconnected nodes. All the circles in our diagram represent the various nodes of our neural networks which are connected to one another. Neural networks are in the form of layers. The first layer is our input layer which contains nothing more than our input tensors. The input tensors will each have a weight associated with them depending on how important they are to our output layer. They did layer in the middle will help find you in the input weightings until the neural networks margin of error is minimal. Each node in our neural network can be taken as a perceptron and is similar to a multiple linear regression. The perceptron feeds the signal which is produced by multiple linear regressions into an activation function which may be nonlinear. The activation function is present within our nodes and depending on the activation function we decide the weightage of our node. Now in tens of flow we know that our data is saved in the form of tensors. These tensors are the input to all of our nodes and all of the tensors are implemented in the form of Python. The computation which is taking place within our nodes, the activation functions and the hidden layer functions are all implemented in C++. The reason we use C++ is because C++ is relatively faster than Python. This makes implementing our neural network using tensors flow a very fast process. For more videos like this and for more information please visit www.simpliland.com.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the transcripts dataset\n",
        "transcripts_df = pd.read_csv('transcripts.csv')\n",
        "\n",
        "# Preprocess the transcripts to get a list of sentences\n",
        "def preprocess_transcripts(transcripts):\n",
        "    # Load stopwords set\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = []\n",
        "    for transcript in transcripts:\n",
        "        # Tokenize the transcript into sentences\n",
        "        transcript_sentences = sent_tokenize(transcript)\n",
        "        for sentence in transcript_sentences:\n",
        "            # Lowercase the sentence\n",
        "            sentence = sentence.lower()\n",
        "            # Remove punctuation\n",
        "            sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "            # Remove stopwords\n",
        "            sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "sentences = preprocess_transcripts(transcripts_df['transcript'])\n",
        "\n",
        "# Download the pre-trained Word2Vec model\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Function to create sentence embeddings by averaging word embeddings\n",
        "def sentence_embedding(sentence, word_vectors):\n",
        "    words = word_tokenize(sentence)\n",
        "    embeddings = [word_vectors[word] for word in words if word in word_vectors]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word_vectors.vector_size)\n",
        "\n",
        "# Create sentence embeddings for each sentence\n",
        "sentence_embeddings = np.array([sentence_embedding(sentence, word_vectors) for sentence in sentences])\n",
        "\n",
        "# Define your labels here. Adjust the threshold for importance as needed.\n",
        "def define_labels(sentences, threshold=10):\n",
        "    # Label sentences as 'important' (1) if they are longer than the threshold number of words\n",
        "    return [1 if len(sentence.split()) > threshold else 0 for sentence in sentences]\n",
        "\n",
        "labels = define_labels(sentences)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Read the new transcript from 'transcript.txt'\n",
        "with open('transcript.txt', 'r') as file:\n",
        "    new_transcript = file.read()\n",
        "\n",
        "# Tokenize the new transcript into sentences\n",
        "new_sentences = sent_tokenize(new_transcript)\n",
        "\n",
        "# Create sentence embeddings for the new sentences\n",
        "new_sentence_embeddings = np.array([sentence_embedding(sentence, word_vectors) for sentence in new_sentences])\n",
        "\n",
        "# Predict the 'importance' of each sentence\n",
        "predictions = model.predict(new_sentence_embeddings)\n",
        "\n",
        "# Create a summary by selecting the 'important' sentences\n",
        "summary_sentences = [sentence for sentence, prediction in zip(new_sentences, predictions) if prediction == 1]\n",
        "summary = ' '.join(summary_sentences)\n",
        "\n",
        "# Save the summary to a text file\n",
        "with open('summary.txt', 'w') as file:\n",
        "    file.write(summary)\n",
        "\n",
        "# Optionally, output the summary to the console\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEIuvio3x_A2",
        "outputId": "55ded0ee-cf23-4205-d4d7-5e1c3e972bc0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So what's in it for you? Finally, we'll be looking at some companies which use TensorFlow in their day-to-day working and we look at the different applications in which they use TensorFlow. Then we'll be taking a look at neural networks with TensorFlow. The tensors can be of any dimension and using tensors we can perform linear operations on vast quantities of data. Using TensorFlow you can train multiple neural networks parallelly. Instead you have computational graphs which are built in processes which use the library without needing to call a graph object directly. Now let's take a look at some of the companies which use TensorFlow and the various operations in which they've implemented TensorFlow. Advances in artificial intelligence and the maturity of TensorFlow enabled the Coca-Cola company to finally achieve a long-sought after frictionless proof of purchase capability on their mobile app by using TensorFlow. Airbus also uses TensorFlow and they use it to extract information from satellite images and deliver valuable insights to their clients. This work has resulted in up to 2.8 times performance improvement which benefits the TensorFlow community and a wide range of customers using TensorFlow on Intel platforms. Using TensorFlow deep transfer learning and generative modeling, PayPal has been able to recognize complex, temporarily varying fraud patterns to help increase their fraud decline accuracy while improving experience of legitimate users through increased precision and identification. Now let's dive into neural networks with TensorFlow. They further allow TensorFlow to perform lazy computing. The operators here are multiplication and addition. Now let's look into neural networks. A neural network will contain layers of interconnected nodes. All the circles in our diagram represent the various nodes of our neural networks which are connected to one another. The input tensors will each have a weight associated with them depending on how important they are to our output layer. Each node in our neural network can be taken as a perceptron and is similar to a multiple linear regression. The perceptron feeds the signal which is produced by multiple linear regressions into an activation function which may be nonlinear. The computation which is taking place within our nodes, the activation functions and the hidden layer functions are all implemented in C++. This makes implementing our neural network using tensors flow a very fast process. For more videos like this and for more information please visit www.simpliland.com.\n"
          ]
        }
      ]
    }
  ]
}